
ğŸ§  Excited to share CORE-Bench - my new LLM reasoning benchmark on Kaggle!

I created a Comprehensive Ordered Reasoning Evaluation (CORE) benchmark to test how well AI models really think.

ğŸ“Š THE RESULTS ARE IN:

ğŸ¥‡ Top Performers (96.1% accuracy):
   â€¢ Gemini 3 Pro Preview
   â€¢ Gemini 3 Flash Preview  
   â€¢ DeepSeek V3.2
   â€¢ Qwen 3 Next 80B

ğŸ¥ˆ Close Behind (94.1% accuracy):
   â€¢ Claude Haiku 4.5

ğŸ¯ WHAT CORE-BENCH TESTS:

âœ… Logical Deduction (15 problems)
   Syllogisms, modus ponens, fallacy detection

âœ… Mathematical Reasoning (15 problems)
   Multi-step word problems, percentages, geometry

âœ… Causal Reasoning (15 problems)
   Cause-effect, counterfactuals, correlation vs causation

âœ… Analogical Reasoning (6 problems)
   Pattern completion (A:B :: C:?)

ğŸ” KEY FINDINGS:

1ï¸âƒ£ Multi-step planning is a differentiator - Claude Haiku 4.5 struggled with the river crossing puzzle

2ï¸âƒ£ Reasoning quality evaluation reveals gaps - stricter judge-based criteria caught issues in top models

3ï¸âƒ£ Core reasoning tasks are becoming saturated - all models aced logic, math, causal, and analogy tasks

ğŸ’¡ This suggests we need HARDER problems to truly differentiate next-gen AI capabilities.

ğŸ”— Try it yourself: kaggle.com/benchmarks/taiwofeyijimi/core-bench
ğŸ“‚ GitHub: github.com/taiwofeyijimi/core-bench

What reasoning challenges should I add next? ğŸ‘‡

#AI #MachineLearning #LLM #Kaggle #Benchmark #DeepLearning #ArtificialIntelligence #DataScience
