{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc50476",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02406b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle_benchmarks as kbench\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import re\n",
    "\n",
    "# Check available models\n",
    "print(\"Available models:\")\n",
    "print(list(kbench.llms.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64748052",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Logical Deduction (Syllogisms)\n",
    "\n",
    "Tests the model's ability to apply logical rules and deduce conclusions from premises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b37c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for logical deduction\n",
    "logic_problems = pd.DataFrame([\n",
    "    # Original problems\n",
    "    {\n",
    "        \"premises\": \"All mammals are warm-blooded. All dogs are mammals.\",\n",
    "        \"question\": \"Are all dogs warm-blooded?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"explanation\": \"Dogs are mammals, and all mammals are warm-blooded, so dogs must be warm-blooded.\"\n",
    "    },\n",
    "    {\n",
    "        \"premises\": \"No reptiles are mammals. All snakes are reptiles.\",\n",
    "        \"question\": \"Are any snakes mammals?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"explanation\": \"Snakes are reptiles, and no reptiles are mammals, so snakes cannot be mammals.\"\n",
    "    },\n",
    "    {\n",
    "        \"premises\": \"All programmers use computers. Some artists are programmers.\",\n",
    "        \"question\": \"Do some artists use computers?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"explanation\": \"Since some artists are programmers, and all programmers use computers, those artists must use computers.\"\n",
    "    },\n",
    "    {\n",
    "        \"premises\": \"All squares are rectangles. All rectangles have four sides.\",\n",
    "        \"question\": \"Do all squares have four sides?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"explanation\": \"Squares are rectangles, rectangles have four sides, therefore squares have four sides.\"\n",
    "    },\n",
    "    {\n",
    "        \"premises\": \"If it rains, the ground gets wet. The ground is wet.\",\n",
    "        \"question\": \"Did it definitely rain?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"explanation\": \"The ground being wet doesn't prove rain - it could have other causes (sprinklers, etc.). This is the fallacy of affirming the consequent.\"\n",
    "    },\n",
    "    # New problems - Modus Ponens\n",
    "    {\n",
    "        \"premises\": \"If a person is a bachelor, then they are unmarried. John is a bachelor.\",\n",
    "        \"question\": \"Is John unmarried?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"explanation\": \"By modus ponens: If P then Q, P is true, therefore Q must be true.\"\n",
    "    },\n",
    "    # Modus Tollens\n",
    "    {\n",
    "        \"premises\": \"If it is a weekday, the office is open. The office is not open.\",\n",
    "        \"question\": \"Is it a weekday?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"explanation\": \"By modus tollens: If P then Q, Q is false, therefore P must be false.\"\n",
    "    },\n",
    "    # Disjunctive Syllogism\n",
    "    {\n",
    "        \"premises\": \"Either the package was delivered or it was lost. The package was not delivered.\",\n",
    "        \"question\": \"Was the package lost?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"explanation\": \"By disjunctive syllogism: Either A or B, not A, therefore B.\"\n",
    "    },\n",
    "    # Hypothetical Syllogism\n",
    "    {\n",
    "        \"premises\": \"If it snows, the schools close. If the schools close, children stay home.\",\n",
    "        \"question\": \"If it snows, do children stay home?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"explanation\": \"By hypothetical syllogism: If A then B, If B then C, therefore If A then C.\"\n",
    "    },\n",
    "    # Existential Fallacy\n",
    "    {\n",
    "        \"premises\": \"All unicorns have horns. All unicorns are magical creatures.\",\n",
    "        \"question\": \"Are there magical creatures with horns?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"explanation\": \"This is the existential fallacy - we cannot conclude existence from universal statements about possibly empty sets.\"\n",
    "    },\n",
    "    # Complex Syllogism\n",
    "    {\n",
    "        \"premises\": \"No honest politician takes bribes. Some senators are honest politicians.\",\n",
    "        \"question\": \"Do some senators not take bribes?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"explanation\": \"Some senators are honest politicians, and no honest politician takes bribes, so those senators don't take bribes.\"\n",
    "    },\n",
    "    # Denying the Antecedent (Fallacy)\n",
    "    {\n",
    "        \"premises\": \"If you study hard, you will pass the exam. You did not study hard.\",\n",
    "        \"question\": \"Can we conclude you will fail the exam?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"explanation\": \"This is the fallacy of denying the antecedent. Not studying hard doesn't guarantee failure - you might pass anyway.\"\n",
    "    },\n",
    "    # Contraposition\n",
    "    {\n",
    "        \"premises\": \"All birds have feathers. Penguins are birds.\",\n",
    "        \"question\": \"Do penguins have feathers?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"explanation\": \"Penguins are birds, all birds have feathers, therefore penguins have feathers.\"\n",
    "    },\n",
    "    # Exclusive Disjunction\n",
    "    {\n",
    "        \"premises\": \"A number is either even or odd, but not both. The number 7 is not even.\",\n",
    "        \"question\": \"Is 7 odd?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"explanation\": \"Since 7 is not even, and a number must be either even or odd, 7 must be odd.\"\n",
    "    },\n",
    "    # Universal Negative\n",
    "    {\n",
    "        \"premises\": \"No fish can breathe air directly. Salmon are fish.\",\n",
    "        \"question\": \"Can salmon breathe air directly?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"explanation\": \"Salmon are fish, and no fish can breathe air directly, so salmon cannot breathe air directly.\"\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"Logical deduction dataset: {len(logic_problems)} problems\")\n",
    "logic_problems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Logical Deduction Problems (10 new harder problems)\n",
    "advanced_logic_problems = pd.DataFrame([\n",
    "    {\"premises\": \"A shape is a square if and only if it has four equal sides and four right angles. This shape has four equal sides and four right angles.\", \"question\": \"Is this shape a square?\", \"answer\": \"yes\", \"explanation\": \"Biconditional: If and only if means both directions hold.\"},\n",
    "    {\"premises\": \"If it's Monday, then if it's raining, the market is closed. It's Monday and it's raining.\", \"question\": \"Is the market closed?\", \"answer\": \"yes\", \"explanation\": \"Nested conditional with both antecedents satisfied.\"},\n",
    "    {\"premises\": \"If I study, I'll pass. If I work, I'll earn money. I will either study or work.\", \"question\": \"Will I either pass or earn money?\", \"answer\": \"yes\", \"explanation\": \"Constructive dilemma: (P→Q) ∧ (R→S) ∧ (P∨R) → (Q∨S)\"},\n",
    "    {\"premises\": \"If the car starts, the battery is charged. If the lights work, the battery is charged. The battery is not charged.\", \"question\": \"Can we conclude neither the car starts nor the lights work?\", \"answer\": \"yes\", \"explanation\": \"Destructive dilemma: contrapositive of both conditionals.\"},\n",
    "    {\"premises\": \"All A are B. All B are C. All C are D. All D are E. X is an A.\", \"question\": \"Is X an E?\", \"answer\": \"yes\", \"explanation\": \"Sorites: transitive chain through multiple categories.\"},\n",
    "    {\"premises\": \"All cats are mammals. No non-mammals are cats.\", \"question\": \"Are these two statements logically equivalent?\", \"answer\": \"yes\", \"explanation\": \"Obversion: 'All S are P' is equivalent to 'No S are non-P'.\"},\n",
    "    {\"premises\": \"All dogs are animals. No cats are dogs.\", \"question\": \"Can we conclude that no cats are animals?\", \"answer\": \"no\", \"explanation\": \"Illicit major fallacy: the major term is undistributed in premise but distributed in conclusion.\"},\n",
    "    {\"premises\": \"All cats are mammals. All dogs are mammals.\", \"question\": \"Can we conclude anything about the relationship between cats and dogs?\", \"answer\": \"no\", \"explanation\": \"Undistributed middle: the middle term 'mammals' is not distributed in either premise.\"},\n",
    "    {\"premises\": \"Every person loves someone. There is someone who is loved by every person.\", \"question\": \"Do these two statements mean the same thing?\", \"answer\": \"no\", \"explanation\": \"Different quantifier scope: ∀x∃y vs ∃y∀x - the first allows different people for each person.\"},\n",
    "    {\"premises\": \"It is necessary that if it rains, the ground gets wet. It is possible that it rains.\", \"question\": \"Is it possible that the ground gets wet?\", \"answer\": \"yes\", \"explanation\": \"Modal logic: necessary conditional + possible antecedent → possible consequent.\"},\n",
    "])\n",
    "\n",
    "# Combine original and advanced logic problems\n",
    "logic_problems = pd.concat([logic_problems, advanced_logic_problems], ignore_index=True)\n",
    "print(f\"Extended logical deduction dataset: {len(logic_problems)} problems (15 original + 10 advanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LogicalAnswer:\n",
    "    \"\"\"Structured response for logical deduction.\"\"\"\n",
    "    answer: str  # \"yes\" or \"no\"\n",
    "    reasoning: str  # Step-by-step reasoning\n",
    "\n",
    "\n",
    "@kbench.task(name=\"logical_deduction\")\n",
    "def logical_deduction_task(llm, premises: str, question: str, answer: str, explanation: str) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluate LLM's ability to perform logical deduction from given premises.\n",
    "    \n",
    "    The model must:\n",
    "    1. Understand the logical structure of the premises\n",
    "    2. Apply valid logical rules\n",
    "    3. Arrive at the correct conclusion\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a logical reasoning expert. Given the following premises, answer the question.\n",
    "\n",
    "PREMISES:\n",
    "{premises}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Think step-by-step and provide your answer as either \"yes\" or \"no\", along with your reasoning.\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.prompt(prompt, schema=LogicalAnswer)\n",
    "    \n",
    "    # Normalize the answer\n",
    "    model_answer = response.answer.lower().strip()\n",
    "    expected_answer = answer.lower().strip()\n",
    "    \n",
    "    is_correct = model_answer == expected_answer\n",
    "    \n",
    "    # Record assertion for visibility\n",
    "    kbench.assertions.assert_true(\n",
    "        is_correct,\n",
    "        expectation=f\"Model answered '{model_answer}', expected '{expected_answer}'. Correct reasoning: {explanation}\"\n",
    "    )\n",
    "    \n",
    "    return is_correct\n",
    "\n",
    "\n",
    "# Test with a single example\n",
    "logical_deduction_task.run(\n",
    "    llm=kbench.llm,\n",
    "    premises=\"All mammals are warm-blooded. All dogs are mammals.\",\n",
    "    question=\"Are all dogs warm-blooded?\",\n",
    "    answer=\"yes\",\n",
    "    explanation=\"Dogs are mammals, and all mammals are warm-blooded.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58287451",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Mathematical Word Problems\n",
    "\n",
    "Tests multi-step mathematical reasoning with real-world context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for math word problems\n",
    "math_problems = pd.DataFrame([\n",
    "    # Original problems\n",
    "    {\n",
    "        \"problem\": \"A store sells apples for $2 each and oranges for $3 each. If Sarah buys 4 apples and 5 oranges, and pays with a $50 bill, how much change does she receive?\",\n",
    "        \"answer\": 27,\n",
    "        \"steps\": \"Apples: 4×$2=$8, Oranges: 5×$3=$15, Total: $8+$15=$23, Change: $50-$23=$27\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A train travels at 60 mph for 2 hours, then at 80 mph for 1.5 hours. What is the total distance traveled?\",\n",
    "        \"answer\": 240,\n",
    "        \"steps\": \"First leg: 60×2=120 miles, Second leg: 80×1.5=120 miles, Total: 120+120=240 miles\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If 3 workers can complete a job in 12 days, how many days would it take 4 workers to complete the same job, assuming they work at the same rate?\",\n",
    "        \"answer\": 9,\n",
    "        \"steps\": \"Total work = 3×12 = 36 worker-days, With 4 workers: 36÷4 = 9 days\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A rectangle has a perimeter of 36 cm. If its length is twice its width, what is the area of the rectangle in square centimeters?\",\n",
    "        \"answer\": 72,\n",
    "        \"steps\": \"Let width=w, length=2w. Perimeter: 2(w+2w)=36, so 6w=36, w=6. Length=12. Area=6×12=72\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A tank is 1/3 full. After adding 40 liters, it becomes 2/3 full. What is the total capacity of the tank?\",\n",
    "        \"answer\": 120,\n",
    "        \"steps\": \"40 liters fills 2/3 - 1/3 = 1/3 of the tank. So full capacity = 40 × 3 = 120 liters\"\n",
    "    },\n",
    "    # New problems - Percentage\n",
    "    {\n",
    "        \"problem\": \"A shirt originally costs $80. It's on sale for 25% off. What is the sale price?\",\n",
    "        \"answer\": 60,\n",
    "        \"steps\": \"Discount: 80 × 0.25 = $20, Sale price: $80 - $20 = $60\"\n",
    "    },\n",
    "    # Compound Interest (simple)\n",
    "    {\n",
    "        \"problem\": \"You invest $1000 at 5% simple interest per year. How much money will you have after 3 years?\",\n",
    "        \"answer\": 1150,\n",
    "        \"steps\": \"Interest per year: 1000 × 0.05 = $50, Total interest: $50 × 3 = $150, Final: $1000 + $150 = $1150\"\n",
    "    },\n",
    "    # Age Problem\n",
    "    {\n",
    "        \"problem\": \"Tom is twice as old as Jerry. In 5 years, Tom will be 1.5 times as old as Jerry. How old is Jerry now?\",\n",
    "        \"answer\": 10,\n",
    "        \"steps\": \"Let Jerry=x, Tom=2x. In 5 years: 2x+5 = 1.5(x+5), 2x+5 = 1.5x+7.5, 0.5x = 2.5, x = 5. Wait, recalculating: 2x+5 = 1.5(x+5) → 2x+5 = 1.5x+7.5 → 0.5x = 2.5 → x = 5. Jerry is 5... Actually let me redo: If Jerry=10, Tom=20. In 5 years: Jerry=15, Tom=25. 25/15 = 1.67 ≠ 1.5. Let's solve: 2x+5 = 1.5(x+5) → 2x+5 = 1.5x+7.5 → 0.5x = 2.5 → x=5\"\n",
    "    },\n",
    "    # Distance/Speed/Time\n",
    "    {\n",
    "        \"problem\": \"Two cars start from the same point traveling in opposite directions. Car A travels at 50 mph and Car B at 70 mph. After how many hours will they be 360 miles apart?\",\n",
    "        \"answer\": 3,\n",
    "        \"steps\": \"Combined speed: 50 + 70 = 120 mph, Time: 360 ÷ 120 = 3 hours\"\n",
    "    },\n",
    "    # Ratio Problem\n",
    "    {\n",
    "        \"problem\": \"The ratio of boys to girls in a class is 3:5. If there are 24 students total, how many girls are there?\",\n",
    "        \"answer\": 15,\n",
    "        \"steps\": \"Total parts: 3+5=8, Each part: 24÷8=3 students, Girls: 5×3=15\"\n",
    "    },\n",
    "    # Mixture Problem\n",
    "    {\n",
    "        \"problem\": \"How many liters of a 20% salt solution must be mixed with 10 liters of a 50% salt solution to get a 30% salt solution?\",\n",
    "        \"answer\": 20,\n",
    "        \"steps\": \"Let x = liters of 20% solution. 0.2x + 0.5(10) = 0.3(x+10), 0.2x + 5 = 0.3x + 3, 2 = 0.1x, x = 20\"\n",
    "    },\n",
    "    # Profit/Loss\n",
    "    {\n",
    "        \"problem\": \"A merchant buys an item for $200 and sells it for $250. What is the profit percentage?\",\n",
    "        \"answer\": 25,\n",
    "        \"steps\": \"Profit: $250 - $200 = $50, Profit %: (50/200) × 100 = 25%\"\n",
    "    },\n",
    "    # Geometry - Circle\n",
    "    {\n",
    "        \"problem\": \"A circular garden has a radius of 7 meters. What is its area in square meters? (Use π = 22/7)\",\n",
    "        \"answer\": 154,\n",
    "        \"steps\": \"Area = πr² = (22/7) × 7² = (22/7) × 49 = 22 × 7 = 154 sq meters\"\n",
    "    },\n",
    "    # Average\n",
    "    {\n",
    "        \"problem\": \"The average of 5 numbers is 20. If one number is removed, the average becomes 15. What number was removed?\",\n",
    "        \"answer\": 40,\n",
    "        \"steps\": \"Sum of 5 numbers: 5 × 20 = 100, Sum of 4 numbers: 4 × 15 = 60, Removed number: 100 - 60 = 40\"\n",
    "    },\n",
    "    # Sequence\n",
    "    {\n",
    "        \"problem\": \"What is the sum of the first 10 positive even numbers?\",\n",
    "        \"answer\": 110,\n",
    "        \"steps\": \"Even numbers: 2,4,6,8,10,12,14,16,18,20. Sum = n(n+1) where n=10: 10×11 = 110. Or: 2+4+6+8+10+12+14+16+18+20 = 110\"\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"Math problems dataset: {len(math_problems)} problems\")\n",
    "math_problems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527195a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Math Problems (10 new harder problems)\n",
    "advanced_math_problems = pd.DataFrame([\n",
    "    {\"problem\": \"A farmer has 100 meters of fencing. What is the maximum rectangular area (in square meters) that can be enclosed?\", \"answer\": 625, \"steps\": \"For max area, rectangle should be square. Perimeter=100, side=25, area=25²=625\"},\n",
    "    {\"problem\": \"A bag contains 3 red balls and 2 blue balls. If two balls are drawn without replacement, what is the probability (as a percentage) that both are red?\", \"answer\": 30, \"steps\": \"P(both red) = (3/5) × (2/4) = 6/20 = 0.30 = 30%\"},\n",
    "    {\"problem\": \"A bacteria colony doubles every 4 hours. If there are 100 bacteria initially, how many will there be after 12 hours?\", \"answer\": 800, \"steps\": \"12 hours = 3 doubling periods. 100 × 2³ = 100 × 8 = 800\"},\n",
    "    {\"problem\": \"How many different 3-letter arrangements can be made from the letters A, B, C, D, E if no letter can be repeated?\", \"answer\": 60, \"steps\": \"Permutation P(5,3) = 5 × 4 × 3 = 60\"},\n",
    "    {\"problem\": \"What is the greatest common divisor (GCD) of 84 and 126?\", \"answer\": 42, \"steps\": \"84 = 2² × 3 × 7, 126 = 2 × 3² × 7. GCD = 2 × 3 × 7 = 42\"},\n",
    "    {\"problem\": \"If log₁₀(x) = 3, what is x?\", \"answer\": 1000, \"steps\": \"log₁₀(x) = 3 means 10³ = x, so x = 1000\"},\n",
    "    {\"problem\": \"In a right triangle, one leg is 3 cm and the hypotenuse is 5 cm. What is the length of the other leg in cm?\", \"answer\": 4, \"steps\": \"Pythagorean theorem: 3² + b² = 5², 9 + b² = 25, b² = 16, b = 4\"},\n",
    "    {\"problem\": \"Simplify: (x² - 9)/(x - 3) when x = 5. What is the result?\", \"answer\": 8, \"steps\": \"(x² - 9)/(x - 3) = (x+3)(x-3)/(x-3) = x+3. When x=5: 5+3=8\"},\n",
    "    {\"problem\": \"A ball is thrown upward with initial velocity 40 m/s. Using h = 40t - 5t², at what time t (in seconds) does it reach maximum height?\", \"answer\": 4, \"steps\": \"Maximum at vertex: t = -b/(2a) = -40/(2×-5) = 40/10 = 4 seconds\"},\n",
    "    {\"problem\": \"What is the sum of the interior angles of a hexagon in degrees?\", \"answer\": 720, \"steps\": \"Sum = (n-2) × 180 = (6-2) × 180 = 4 × 180 = 720 degrees\"},\n",
    "])\n",
    "\n",
    "# Combine original and advanced math problems\n",
    "math_problems = pd.concat([math_problems, advanced_math_problems], ignore_index=True)\n",
    "print(f\"Extended math dataset: {len(math_problems)} problems (15 original + 10 advanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MathSolution:\n",
    "    \"\"\"Structured response for math problems.\"\"\"\n",
    "    step_by_step: str  # Detailed solution steps\n",
    "    final_answer: float  # Numerical answer\n",
    "\n",
    "\n",
    "@kbench.task(name=\"math_word_problems\")\n",
    "def math_reasoning_task(llm, problem: str, answer: float, steps: str) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluate LLM's math reasoning: extract info, set up equations, calculate accurately, and provide correct numerical answers.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Solve this math word problem step by step.\n",
    "\n",
    "PROBLEM:\n",
    "{problem}\n",
    "\n",
    "Show your work clearly, then provide the final numerical answer.\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.prompt(prompt, schema=MathSolution)\n",
    "    \n",
    "    # Allow small floating point tolerance\n",
    "    is_correct = abs(response.final_answer - answer) < 0.01\n",
    "    \n",
    "    kbench.assertions.assert_true(\n",
    "        is_correct,\n",
    "        expectation=f\"Model answered {response.final_answer}, expected {answer}. Correct steps: {steps}\"\n",
    "    )\n",
    "    \n",
    "    return is_correct\n",
    "\n",
    "\n",
    "# Test with a single example\n",
    "math_reasoning_task.run(\n",
    "    llm=kbench.llm,\n",
    "    problem=\"A store sells apples for $2 each and oranges for $3 each. If Sarah buys 4 apples and 5 oranges, and pays with a $50 bill, how much change does she receive?\",\n",
    "    answer=27,\n",
    "    steps=\"Apples: 4×$2=$8, Oranges: 5×$3=$15, Total: $23, Change: $27\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f38685",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Causal Reasoning\n",
    "\n",
    "Tests understanding of cause-and-effect relationships and counterfactual thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18263407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for causal reasoning\n",
    "causal_problems = pd.DataFrame([\n",
    "    # Original problems\n",
    "    {\n",
    "        \"scenario\": \"John was late to work because his alarm didn't go off. His alarm didn't go off because there was a power outage overnight.\",\n",
    "        \"question\": \"What was the root cause of John being late?\",\n",
    "        \"answer\": \"power outage\",\n",
    "        \"reasoning_type\": \"causal chain\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Plants in Garden A received fertilizer and grew 30% taller than plants in Garden B which received no fertilizer. Both gardens had identical soil, sunlight, and water.\",\n",
    "        \"question\": \"What caused the difference in plant height?\",\n",
    "        \"answer\": \"fertilizer\",\n",
    "        \"reasoning_type\": \"controlled experiment\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Every time it rains, the street gets wet. The street is currently wet.\",\n",
    "        \"question\": \"Can we conclude it rained?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"reasoning_type\": \"correlation vs causation\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"A factory produces widgets. Machine A breaks down. Production stops. Machine A is repaired. Production resumes.\",\n",
    "        \"question\": \"What would have happened if Machine A hadn't been repaired?\",\n",
    "        \"answer\": \"production would not have resumed\",\n",
    "        \"reasoning_type\": \"counterfactual\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Ice cream sales and drowning incidents both increase in summer. Someone claims eating ice cream causes drowning.\",\n",
    "        \"question\": \"Is this causal claim valid?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"reasoning_type\": \"spurious correlation\"\n",
    "    },\n",
    "    # New problems - Confounding Variable\n",
    "    {\n",
    "        \"scenario\": \"A study finds that people who drink coffee live longer. However, coffee drinkers also tend to be wealthier and have better access to healthcare.\",\n",
    "        \"question\": \"Can we conclude coffee causes longer life?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"reasoning_type\": \"confounding variable\"\n",
    "    },\n",
    "    # Necessary Condition\n",
    "    {\n",
    "        \"scenario\": \"To start a car, you need a key (or key fob). The car started.\",\n",
    "        \"question\": \"Was a key used?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"reasoning_type\": \"necessary condition\"\n",
    "    },\n",
    "    # Sufficient Condition\n",
    "    {\n",
    "        \"scenario\": \"Getting 100% on the final exam guarantees passing the course. Maria passed the course.\",\n",
    "        \"question\": \"Did Maria definitely get 100% on the final exam?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"reasoning_type\": \"sufficient but not necessary\"\n",
    "    },\n",
    "    # Reverse Causation\n",
    "    {\n",
    "        \"scenario\": \"Countries with more hospitals have higher death rates. Someone concludes hospitals cause death.\",\n",
    "        \"question\": \"Is this reasoning correct?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"reasoning_type\": \"reverse causation\"\n",
    "    },\n",
    "    # Multiple Causes\n",
    "    {\n",
    "        \"scenario\": \"A fire requires heat, fuel, and oxygen. A fire started in a warehouse that had all three elements.\",\n",
    "        \"question\": \"Would removing the oxygen have prevented the fire?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"reasoning_type\": \"necessary conditions\"\n",
    "    },\n",
    "    # Natural Experiment\n",
    "    {\n",
    "        \"scenario\": \"Two identical twin cities exist on opposite sides of a border. One city banned smoking in public, the other didn't. After 10 years, the city with the ban had 20% fewer lung cancer cases.\",\n",
    "        \"question\": \"What likely caused the difference in lung cancer rates?\",\n",
    "        \"answer\": \"smoking ban\",\n",
    "        \"reasoning_type\": \"natural experiment\"\n",
    "    },\n",
    "    # Causal Chain (Complex)\n",
    "    {\n",
    "        \"scenario\": \"Deforestation leads to soil erosion. Soil erosion causes rivers to become silted. Silted rivers lead to flooding. A region experienced severe flooding.\",\n",
    "        \"question\": \"Could deforestation be a root cause of the flooding?\",\n",
    "        \"answer\": \"yes\",\n",
    "        \"reasoning_type\": \"complex causal chain\"\n",
    "    },\n",
    "    # Selection Bias\n",
    "    {\n",
    "        \"scenario\": \"A survey of gym members found that 90% exercise regularly. The researcher concluded that 90% of the population exercises regularly.\",\n",
    "        \"question\": \"Is this conclusion valid?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"reasoning_type\": \"selection bias\"\n",
    "    },\n",
    "    # Post Hoc Fallacy\n",
    "    {\n",
    "        \"scenario\": \"I wore my lucky socks and my team won. Therefore, my lucky socks caused the win.\",\n",
    "        \"question\": \"Is this causal reasoning valid?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"reasoning_type\": \"post hoc fallacy\"\n",
    "    },\n",
    "    # Intervention vs Observation\n",
    "    {\n",
    "        \"scenario\": \"Observational data shows that students who sit in the front row get better grades. A student decides to sit in the front row expecting better grades.\",\n",
    "        \"question\": \"Will sitting in the front row definitely improve grades?\",\n",
    "        \"answer\": \"no\",\n",
    "        \"reasoning_type\": \"intervention vs observation\"\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"Causal reasoning dataset: {len(causal_problems)} problems\")\n",
    "causal_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82078b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Causal Reasoning Problems (10 new harder problems)\n",
    "advanced_causal_problems = pd.DataFrame([\n",
    "    {\"scenario\": \"Treatment A has better success rates than Treatment B for both mild and severe cases when analyzed separately. However, when all cases are combined, Treatment B appears more successful overall.\", \"question\": \"Is Treatment B actually better overall?\", \"answer\": \"no\", \"reasoning_type\": \"simpson's paradox\"},\n",
    "    {\"scenario\": \"Education level is associated with higher income. Education also leads to better job positions, and better positions lead to higher income.\", \"question\": \"Is job position a mediating variable between education and income?\", \"answer\": \"yes\", \"reasoning_type\": \"mediating variable\"},\n",
    "    {\"scenario\": \"A coach punishes players after their worst performances. Players tend to perform better after being punished. The coach concludes punishment improves performance.\", \"question\": \"Is this conclusion justified?\", \"answer\": \"no\", \"reasoning_type\": \"regression to the mean\"},\n",
    "    {\"scenario\": \"A researcher studies successful startups and finds they all took big risks. The researcher concludes that taking big risks leads to success.\", \"question\": \"Is this conclusion valid?\", \"answer\": \"no\", \"reasoning_type\": \"survivorship bias\"},\n",
    "    {\"scenario\": \"In a hospital, there appears to be a negative correlation between two diseases - patients with Disease A are less likely to have Disease B. Both diseases independently cause hospitalization.\", \"question\": \"Does Disease A protect against Disease B in the general population?\", \"answer\": \"no\", \"reasoning_type\": \"berkson's paradox\"},\n",
    "    {\"scenario\": \"To study if education causes higher earnings (controlling for motivation), a researcher uses distance to college as an instrument. Distance affects education but doesn't directly affect earnings.\", \"question\": \"Is distance to college a valid instrumental variable?\", \"answer\": \"yes\", \"reasoning_type\": \"instrumental variable\"},\n",
    "    {\"scenario\": \"Increased police presence leads to more arrests. More arrests lead to increased crime statistics. Higher crime statistics lead to more police funding.\", \"question\": \"Does this prove that more police causes more crime?\", \"answer\": \"no\", \"reasoning_type\": \"feedback loop\"},\n",
    "    {\"scenario\": \"Countries with higher chocolate consumption have more Nobel Prize winners per capita. Therefore, eating chocolate makes individuals smarter.\", \"question\": \"Is this individual-level conclusion valid?\", \"answer\": \"no\", \"reasoning_type\": \"ecological fallacy\"},\n",
    "    {\"scenario\": \"A study finds that firefighters are present at larger fires. Conclusion: firefighters cause fires to be larger.\", \"question\": \"Is this causal claim valid?\", \"answer\": \"no\", \"reasoning_type\": \"omitted variable bias\"},\n",
    "    {\"scenario\": \"In a study, depression was measured at the same time as social media use. High social media use correlated with more depression.\", \"question\": \"Can we conclude social media causes depression?\", \"answer\": \"no\", \"reasoning_type\": \"temporal precedence\"},\n",
    "])\n",
    "\n",
    "# Combine original and advanced causal problems\n",
    "causal_problems = pd.concat([causal_problems, advanced_causal_problems], ignore_index=True)\n",
    "print(f\"Extended causal reasoning dataset: {len(causal_problems)} problems (15 original + 10 advanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kbench.task(name=\"causal_reasoning\")\n",
    "def causal_reasoning_task(llm, scenario: str, question: str, answer: str, reasoning_type: str) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluate LLM's ability to understand causal relationships.\n",
    "    \n",
    "    Tests:\n",
    "    - Causal chain identification\n",
    "    - Distinguishing correlation from causation\n",
    "    - Counterfactual reasoning\n",
    "    - Identifying spurious correlations\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Analyze the following scenario and answer the question about causality.\n",
    "\n",
    "SCENARIO:\n",
    "{scenario}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Provide a clear, concise answer with brief reasoning.\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.prompt(prompt)\n",
    "    \n",
    "    # Check if the key answer concept is present\n",
    "    is_correct = answer.lower() in response.lower()\n",
    "    \n",
    "    kbench.assertions.assert_true(\n",
    "        is_correct,\n",
    "        expectation=f\"[{reasoning_type}] Expected answer to contain '{answer}'. Model response: {response[:200]}...\"\n",
    "    )\n",
    "    \n",
    "    return is_correct\n",
    "\n",
    "\n",
    "# Test with a single example\n",
    "causal_reasoning_task.run(\n",
    "    llm=kbench.llm,\n",
    "    scenario=\"Ice cream sales and drowning incidents both increase in summer. Someone claims eating ice cream causes drowning.\",\n",
    "    question=\"Is this causal claim valid?\",\n",
    "    answer=\"no\",\n",
    "    reasoning_type=\"spurious correlation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89266ed",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Analogical Reasoning\n",
    "\n",
    "Tests the ability to identify patterns and complete analogies (A:B :: C:?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for analogical reasoning\n",
    "analogy_problems = pd.DataFrame([\n",
    "    {\n",
    "        \"analogy\": \"Painter : Canvas :: Writer : ?\",\n",
    "        \"options\": [\"Book\", \"Paper\", \"Pen\", \"Story\"],\n",
    "        \"answer\": \"Paper\",\n",
    "        \"relationship\": \"Artist : Medium they work on\"\n",
    "    },\n",
    "    {\n",
    "        \"analogy\": \"Fish : Swim :: Bird : ?\",\n",
    "        \"options\": [\"Feather\", \"Fly\", \"Nest\", \"Beak\"],\n",
    "        \"answer\": \"Fly\",\n",
    "        \"relationship\": \"Animal : Primary mode of locomotion\"\n",
    "    },\n",
    "    {\n",
    "        \"analogy\": \"Chapter : Book :: Scene : ?\",\n",
    "        \"options\": [\"Movie\", \"Play\", \"Actor\", \"Stage\"],\n",
    "        \"answer\": \"Play\",\n",
    "        \"relationship\": \"Component : Larger work it belongs to\"\n",
    "    },\n",
    "    {\n",
    "        \"analogy\": \"Thermometer : Temperature :: Speedometer : ?\",\n",
    "        \"options\": [\"Car\", \"Velocity\", \"Dashboard\", \"Needle\"],\n",
    "        \"answer\": \"Velocity\",\n",
    "        \"relationship\": \"Instrument : What it measures\"\n",
    "    },\n",
    "    {\n",
    "        \"analogy\": \"Caterpillar : Butterfly :: Tadpole : ?\",\n",
    "        \"options\": [\"Fish\", \"Pond\", \"Frog\", \"Egg\"],\n",
    "        \"answer\": \"Frog\",\n",
    "        \"relationship\": \"Juvenile form : Adult form\"\n",
    "    },\n",
    "    {\n",
    "        \"analogy\": \"Doctor : Hospital :: Teacher : ?\",\n",
    "        \"options\": [\"Student\", \"School\", \"Classroom\", \"Education\"],\n",
    "        \"answer\": \"School\",\n",
    "        \"relationship\": \"Professional : Their workplace\"\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"Analogy dataset: {len(analogy_problems)} problems\")\n",
    "analogy_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6550ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analogical Reasoning Problems (10 new problems)\n",
    "advanced_analogy_problems = pd.DataFrame([\n",
    "    {\"analogy\": \"Sword : Knight :: Stethoscope : ?\", \"options\": [\"Hospital\", \"Doctor\", \"Patient\", \"Medicine\"], \"answer\": \"Doctor\", \"relationship\": \"Tool : Professional who uses it\"},\n",
    "    {\"analogy\": \"Telescope : Stars :: Microscope : ?\", \"options\": [\"Bacteria\", \"Laboratory\", \"Scientist\", \"Glass\"], \"answer\": \"Bacteria\", \"relationship\": \"Instrument : What it's used to observe\"},\n",
    "    {\"analogy\": \"Conductor : Orchestra :: Director : ?\", \"options\": [\"Film\", \"Actor\", \"Camera\", \"Script\"], \"answer\": \"Film\", \"relationship\": \"Leader : What they lead/create\"},\n",
    "    {\"analogy\": \"Hunger : Eat :: Thirst : ?\", \"options\": [\"Water\", \"Drink\", \"Throat\", \"Dehydration\"], \"answer\": \"Drink\", \"relationship\": \"Sensation : Action to satisfy it\"},\n",
    "    {\"analogy\": \"Seed : Tree :: Egg : ?\", \"options\": [\"Nest\", \"Bird\", \"Shell\", \"Hatch\"], \"answer\": \"Bird\", \"relationship\": \"Beginning stage : Fully developed form\"},\n",
    "    {\"analogy\": \"Petal : Flower :: Feather : ?\", \"options\": [\"Wing\", \"Bird\", \"Flight\", \"Nest\"], \"answer\": \"Bird\", \"relationship\": \"Component : The whole organism\"},\n",
    "    {\"analogy\": \"Verse : Poem :: Movement : ?\", \"options\": [\"Dance\", \"Symphony\", \"Orchestra\", \"Conductor\"], \"answer\": \"Symphony\", \"relationship\": \"Section : Complete artistic work\"},\n",
    "    {\"analogy\": \"Author : Novel :: Composer : ?\", \"options\": [\"Music\", \"Symphony\", \"Piano\", \"Orchestra\"], \"answer\": \"Symphony\", \"relationship\": \"Creator : Their major work type\"},\n",
    "    {\"analogy\": \"Dilute : Concentrate :: Expand : ?\", \"options\": [\"Contract\", \"Grow\", \"Stretch\", \"Increase\"], \"answer\": \"Contract\", \"relationship\": \"Antonyms/Opposites\"},\n",
    "    {\"analogy\": \"Marathon : Sprint :: Novel : ?\", \"options\": [\"Book\", \"Story\", \"Short Story\", \"Author\"], \"answer\": \"Short Story\", \"relationship\": \"Long form : Short form of same type\"},\n",
    "])\n",
    "\n",
    "# Combine original and advanced analogy problems\n",
    "analogy_problems = pd.concat([analogy_problems, advanced_analogy_problems], ignore_index=True)\n",
    "print(f\"Extended analogy dataset: {len(analogy_problems)} problems (6 original + 10 advanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36115ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AnalogyAnswer:\n",
    "    \"\"\"Structured response for analogy problems.\"\"\"\n",
    "    selected_answer: str\n",
    "    relationship_explanation: str\n",
    "\n",
    "\n",
    "@kbench.task(name=\"analogical_reasoning\")\n",
    "def analogy_task(llm, analogy: str, options: List[str], answer: str, relationship: str) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluate LLM's ability to complete analogies.\n",
    "    \n",
    "    The model must:\n",
    "    1. Identify the relationship between the first pair\n",
    "    2. Apply the same relationship to find the missing term\n",
    "    3. Select the correct answer from options\n",
    "    \"\"\"\n",
    "    options_str = \", \".join(options)\n",
    "    \n",
    "    prompt = f\"\"\"Complete this analogy by selecting the best option.\n",
    "\n",
    "ANALOGY:\n",
    "{analogy}\n",
    "\n",
    "OPTIONS:\n",
    "{options_str}\n",
    "\n",
    "First identify the relationship in the analogy, then select the answer that best completes it.\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.prompt(prompt, schema=AnalogyAnswer)\n",
    "    \n",
    "    is_correct = response.selected_answer.lower().strip() == answer.lower().strip()\n",
    "    \n",
    "    kbench.assertions.assert_true(\n",
    "        is_correct,\n",
    "        expectation=f\"Model selected '{response.selected_answer}', expected '{answer}'. Relationship: {relationship}\"\n",
    "    )\n",
    "    \n",
    "    return is_correct\n",
    "\n",
    "\n",
    "# Test with a single example\n",
    "analogy_task.run(\n",
    "    llm=kbench.llm,\n",
    "    analogy=\"Fish : Swim :: Bird : ?\",\n",
    "    options=[\"Feather\", \"Fly\", \"Nest\", \"Beak\"],\n",
    "    answer=\"Fly\",\n",
    "    relationship=\"Animal : Primary mode of locomotion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5eae22",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Multi-Step Planning (River Crossing Puzzle)\n",
    "\n",
    "Tests complex sequential reasoning with constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ce19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PlanningAnswer:\n",
    "    \"\"\"Structured response for planning problems.\"\"\"\n",
    "    steps: List[str]  # Ordered list of steps\n",
    "    total_crossings: int\n",
    "    explanation: str\n",
    "\n",
    "\n",
    "@kbench.task(name=\"multi_step_planning\")\n",
    "def planning_task(llm) -> bool:\n",
    "    \"\"\"\n",
    "    River crossing puzzle: transport wolf, goat, cabbage across river one at a time without leaving incompatible pairs alone.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"Solve this classic river crossing puzzle:\n",
    "\n",
    "A farmer needs to cross a river with three items: a wolf, a goat, and a cabbage.\n",
    "The farmer has a small boat that can only carry himself and ONE item at a time.\n",
    "\n",
    "CONSTRAINTS:\n",
    "- The wolf cannot be left alone with the goat (the wolf will eat the goat)\n",
    "- The goat cannot be left alone with the cabbage (the goat will eat the cabbage)\n",
    "- The farmer must be present to prevent any eating\n",
    "\n",
    "How can the farmer get all three items across the river safely?\n",
    "\n",
    "Provide the step-by-step solution with each river crossing.\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.prompt(prompt, schema=PlanningAnswer)\n",
    "    \n",
    "    # Validate the solution\n",
    "    # The minimum solution requires 7 crossings\n",
    "    min_crossings = 7\n",
    "    \n",
    "    # Handle steps that may be strings or dicts\n",
    "    steps_list = response.steps if response.steps else []\n",
    "    normalized_steps = []\n",
    "    for step in steps_list:\n",
    "        if isinstance(step, dict):\n",
    "            normalized_steps.append(str(step.get('description', step.get('step', str(step)))))\n",
    "        else:\n",
    "            normalized_steps.append(str(step))\n",
    "    \n",
    "    # Check for key solution elements\n",
    "    solution_text = \" \".join(normalized_steps).lower()\n",
    "    \n",
    "    # The goat must go first (critical insight)\n",
    "    goat_first = \"goat\" in normalized_steps[0].lower() if normalized_steps else False\n",
    "    \n",
    "    # Check if solution addresses all items\n",
    "    has_wolf = \"wolf\" in solution_text\n",
    "    has_goat = \"goat\" in solution_text\n",
    "    has_cabbage = \"cabbage\" in solution_text\n",
    "    \n",
    "    is_valid = goat_first and has_wolf and has_goat and has_cabbage\n",
    "    \n",
    "    kbench.assertions.assert_true(\n",
    "        goat_first,\n",
    "        expectation=\"The goat must be taken across first (critical insight for this puzzle)\"\n",
    "    )\n",
    "    \n",
    "    kbench.assertions.assert_true(\n",
    "        has_wolf and has_goat and has_cabbage,\n",
    "        expectation=\"Solution must address transporting all three items: wolf, goat, and cabbage\"\n",
    "    )\n",
    "    \n",
    "    kbench.assertions.assert_true(\n",
    "        response.total_crossings >= min_crossings,\n",
    "        expectation=f\"Minimum valid solution requires {min_crossings} crossings. Model proposed {response.total_crossings}.\"\n",
    "    )\n",
    "    \n",
    "    return is_valid\n",
    "\n",
    "\n",
    "# Run the planning task\n",
    "planning_task.run(llm=kbench.llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe412f",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: Comprehensive Reasoning Evaluation\n",
    "\n",
    "This is the **main task** that combines all reasoning types and evaluates across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all datasets with task type labels\n",
    "# Define datasets inline to ensure availability in Kaggle Benchmarks execution\n",
    "_logic_problems = pd.DataFrame([\n",
    "    {\"premises\": \"All mammals are warm-blooded. All dogs are mammals.\", \"question\": \"Are all dogs warm-blooded?\", \"answer\": \"yes\", \"explanation\": \"Dogs are mammals, and all mammals are warm-blooded.\"},\n",
    "    {\"premises\": \"No reptiles are mammals. All snakes are reptiles.\", \"question\": \"Are any snakes mammals?\", \"answer\": \"no\", \"explanation\": \"Snakes are reptiles, and no reptiles are mammals.\"},\n",
    "    {\"premises\": \"All programmers use computers. Some artists are programmers.\", \"question\": \"Do some artists use computers?\", \"answer\": \"yes\", \"explanation\": \"Some artists are programmers who use computers.\"},\n",
    "    {\"premises\": \"All squares are rectangles. All rectangles have four sides.\", \"question\": \"Do all squares have four sides?\", \"answer\": \"yes\", \"explanation\": \"Squares are rectangles with four sides.\"},\n",
    "    {\"premises\": \"If it rains, the ground gets wet. The ground is wet.\", \"question\": \"Did it definitely rain?\", \"answer\": \"no\", \"explanation\": \"Affirming the consequent fallacy.\"},\n",
    "    {\"premises\": \"If a person is a bachelor, then they are unmarried. John is a bachelor.\", \"question\": \"Is John unmarried?\", \"answer\": \"yes\", \"explanation\": \"Modus ponens.\"},\n",
    "    {\"premises\": \"If it is a weekday, the office is open. The office is not open.\", \"question\": \"Is it a weekday?\", \"answer\": \"no\", \"explanation\": \"Modus tollens.\"},\n",
    "    {\"premises\": \"Either the package was delivered or it was lost. The package was not delivered.\", \"question\": \"Was the package lost?\", \"answer\": \"yes\", \"explanation\": \"Disjunctive syllogism.\"},\n",
    "    {\"premises\": \"If it snows, the schools close. If the schools close, children stay home.\", \"question\": \"If it snows, do children stay home?\", \"answer\": \"yes\", \"explanation\": \"Hypothetical syllogism.\"},\n",
    "    {\"premises\": \"All unicorns have horns. All unicorns are magical creatures.\", \"question\": \"Are there magical creatures with horns?\", \"answer\": \"no\", \"explanation\": \"Existential fallacy.\"},\n",
    "    {\"premises\": \"No honest politician takes bribes. Some senators are honest politicians.\", \"question\": \"Do some senators not take bribes?\", \"answer\": \"yes\", \"explanation\": \"Valid syllogism.\"},\n",
    "    {\"premises\": \"If you study hard, you will pass the exam. You did not study hard.\", \"question\": \"Can we conclude you will fail the exam?\", \"answer\": \"no\", \"explanation\": \"Denying the antecedent fallacy.\"},\n",
    "    {\"premises\": \"All birds have feathers. Penguins are birds.\", \"question\": \"Do penguins have feathers?\", \"answer\": \"yes\", \"explanation\": \"Valid syllogism.\"},\n",
    "    {\"premises\": \"A number is either even or odd, but not both. The number 7 is not even.\", \"question\": \"Is 7 odd?\", \"answer\": \"yes\", \"explanation\": \"Exclusive disjunction.\"},\n",
    "    {\"premises\": \"No fish can breathe air directly. Salmon are fish.\", \"question\": \"Can salmon breathe air directly?\", \"answer\": \"no\", \"explanation\": \"Universal negative.\"},\n",
    "    # Advanced logic problems\n",
    "    {\"premises\": \"A shape is a square if and only if it has four equal sides and four right angles. This shape has four equal sides and four right angles.\", \"question\": \"Is this shape a square?\", \"answer\": \"yes\", \"explanation\": \"Biconditional.\"},\n",
    "    {\"premises\": \"If it's Monday, then if it's raining, the market is closed. It's Monday and it's raining.\", \"question\": \"Is the market closed?\", \"answer\": \"yes\", \"explanation\": \"Nested conditional.\"},\n",
    "    {\"premises\": \"If I study, I'll pass. If I work, I'll earn money. I will either study or work.\", \"question\": \"Will I either pass or earn money?\", \"answer\": \"yes\", \"explanation\": \"Constructive dilemma.\"},\n",
    "    {\"premises\": \"If the car starts, the battery is charged. If the lights work, the battery is charged. The battery is not charged.\", \"question\": \"Can we conclude neither the car starts nor the lights work?\", \"answer\": \"yes\", \"explanation\": \"Destructive dilemma.\"},\n",
    "    {\"premises\": \"All A are B. All B are C. All C are D. All D are E. X is an A.\", \"question\": \"Is X an E?\", \"answer\": \"yes\", \"explanation\": \"Sorites chain.\"},\n",
    "    {\"premises\": \"All cats are mammals. No non-mammals are cats.\", \"question\": \"Are these two statements logically equivalent?\", \"answer\": \"yes\", \"explanation\": \"Obversion.\"},\n",
    "    {\"premises\": \"All dogs are animals. No cats are dogs.\", \"question\": \"Can we conclude that no cats are animals?\", \"answer\": \"no\", \"explanation\": \"Illicit major fallacy.\"},\n",
    "    {\"premises\": \"All cats are mammals. All dogs are mammals.\", \"question\": \"Can we conclude anything about the relationship between cats and dogs?\", \"answer\": \"no\", \"explanation\": \"Undistributed middle.\"},\n",
    "    {\"premises\": \"Every person loves someone. There is someone who is loved by every person.\", \"question\": \"Do these two statements mean the same thing?\", \"answer\": \"no\", \"explanation\": \"Quantifier scope difference.\"},\n",
    "    {\"premises\": \"It is necessary that if it rains, the ground gets wet. It is possible that it rains.\", \"question\": \"Is it possible that the ground gets wet?\", \"answer\": \"yes\", \"explanation\": \"Modal logic.\"},\n",
    "])\n",
    "\n",
    "_math_problems = pd.DataFrame([\n",
    "    {\"problem\": \"A store sells apples for $2 each and oranges for $3 each. If Sarah buys 4 apples and 5 oranges, and pays with a $50 bill, how much change does she receive?\", \"answer\": 27, \"steps\": \"4×$2=$8, 5×$3=$15, Total=$23, Change=$27\"},\n",
    "    {\"problem\": \"A train travels at 60 mph for 2 hours, then at 80 mph for 1.5 hours. What is the total distance traveled?\", \"answer\": 240, \"steps\": \"60×2=120, 80×1.5=120, Total=240\"},\n",
    "    {\"problem\": \"If 3 workers can complete a job in 12 days, how many days would it take 4 workers?\", \"answer\": 9, \"steps\": \"3×12=36 worker-days, 36÷4=9 days\"},\n",
    "    {\"problem\": \"A rectangle has a perimeter of 36 cm. If its length is twice its width, what is the area?\", \"answer\": 72, \"steps\": \"w=6, l=12, Area=72\"},\n",
    "    {\"problem\": \"A tank is 1/3 full. After adding 40 liters, it becomes 2/3 full. What is the total capacity?\", \"answer\": 120, \"steps\": \"40L = 1/3, capacity=120L\"},\n",
    "    {\"problem\": \"A shirt originally costs $80. It's on sale for 25% off. What is the sale price?\", \"answer\": 60, \"steps\": \"80×0.25=$20, $80-$20=$60\"},\n",
    "    {\"problem\": \"You invest $1000 at 5% simple interest per year. How much after 3 years?\", \"answer\": 1150, \"steps\": \"$50×3=$150, $1000+$150=$1150\"},\n",
    "    {\"problem\": \"Tom is twice as old as Jerry. In 5 years, Tom will be 1.5 times as old as Jerry. How old is Jerry?\", \"answer\": 5, \"steps\": \"2x+5=1.5(x+5), x=5\"},\n",
    "    {\"problem\": \"Two cars travel opposite directions at 50 and 70 mph. When are they 360 miles apart?\", \"answer\": 3, \"steps\": \"120 mph combined, 360÷120=3 hours\"},\n",
    "    {\"problem\": \"The ratio of boys to girls is 3:5. If there are 24 students, how many girls?\", \"answer\": 15, \"steps\": \"24÷8=3, 5×3=15\"},\n",
    "    {\"problem\": \"Mix 20% salt solution with 10L of 50% to get 30%. How many liters of 20%?\", \"answer\": 20, \"steps\": \"0.2x+5=0.3(x+10), x=20\"},\n",
    "    {\"problem\": \"Buy for $200, sell for $250. What is the profit percentage?\", \"answer\": 25, \"steps\": \"50/200×100=25%\"},\n",
    "    {\"problem\": \"A circular garden has radius 7m. What is its area? (π=22/7)\", \"answer\": 154, \"steps\": \"(22/7)×49=154\"},\n",
    "    {\"problem\": \"Average of 5 numbers is 20. Remove one, average becomes 15. What was removed?\", \"answer\": 40, \"steps\": \"100-60=40\"},\n",
    "    {\"problem\": \"What is the sum of the first 10 positive even numbers?\", \"answer\": 110, \"steps\": \"2+4+...+20=110\"},\n",
    "    # Advanced math problems\n",
    "    {\"problem\": \"A farmer has 100 meters of fencing. What is the maximum rectangular area (in square meters) that can be enclosed?\", \"answer\": 625, \"steps\": \"Square side=25, area=625\"},\n",
    "    {\"problem\": \"A bag contains 3 red and 2 blue balls. If two balls drawn without replacement, what is the probability both are red (as percentage)?\", \"answer\": 30, \"steps\": \"(3/5)×(2/4)=30%\"},\n",
    "    {\"problem\": \"A bacteria colony doubles every 4 hours. Starting with 100, how many after 12 hours?\", \"answer\": 800, \"steps\": \"3 doublings: 100×8=800\"},\n",
    "    {\"problem\": \"How many 3-letter arrangements from A,B,C,D,E with no repetition?\", \"answer\": 60, \"steps\": \"5×4×3=60\"},\n",
    "    {\"problem\": \"What is the GCD of 84 and 126?\", \"answer\": 42, \"steps\": \"2×3×7=42\"},\n",
    "    {\"problem\": \"If log₁₀(x) = 3, what is x?\", \"answer\": 1000, \"steps\": \"10³=1000\"},\n",
    "    {\"problem\": \"Right triangle: one leg is 3cm, hypotenuse is 5cm. What is the other leg in cm?\", \"answer\": 4, \"steps\": \"3²+b²=5², b=4\"},\n",
    "    {\"problem\": \"Simplify (x²-9)/(x-3) when x=5. What is the result?\", \"answer\": 8, \"steps\": \"(x+3)=8\"},\n",
    "    {\"problem\": \"Ball thrown up with v=40m/s. Using h=40t-5t², when does it reach max height (seconds)?\", \"answer\": 4, \"steps\": \"t=-40/(2×-5)=4\"},\n",
    "    {\"problem\": \"What is the sum of interior angles of a hexagon in degrees?\", \"answer\": 720, \"steps\": \"(6-2)×180=720\"},\n",
    "])\n",
    "\n",
    "_causal_problems = pd.DataFrame([\n",
    "    {\"scenario\": \"John was late because his alarm didn't go off due to a power outage.\", \"question\": \"What was the root cause?\", \"answer\": \"power outage\", \"reasoning_type\": \"causal chain\"},\n",
    "    {\"scenario\": \"Garden A with fertilizer grew 30% taller than Garden B without. Same soil, sun, water.\", \"question\": \"What caused the difference?\", \"answer\": \"fertilizer\", \"reasoning_type\": \"controlled experiment\"},\n",
    "    {\"scenario\": \"Every time it rains, the street gets wet. The street is wet.\", \"question\": \"Can we conclude it rained?\", \"answer\": \"no\", \"reasoning_type\": \"correlation vs causation\"},\n",
    "    {\"scenario\": \"Machine A breaks, production stops. Machine A repaired, production resumes.\", \"question\": \"What if Machine A wasn't repaired?\", \"answer\": \"production would not have resumed\", \"reasoning_type\": \"counterfactual\"},\n",
    "    {\"scenario\": \"Ice cream sales and drowning both increase in summer. Claim: ice cream causes drowning.\", \"question\": \"Is this causal claim valid?\", \"answer\": \"no\", \"reasoning_type\": \"spurious correlation\"},\n",
    "    {\"scenario\": \"Coffee drinkers live longer but are also wealthier with better healthcare.\", \"question\": \"Can we conclude coffee causes longer life?\", \"answer\": \"no\", \"reasoning_type\": \"confounding variable\"},\n",
    "    {\"scenario\": \"To start a car, you need a key. The car started.\", \"question\": \"Was a key used?\", \"answer\": \"yes\", \"reasoning_type\": \"necessary condition\"},\n",
    "    {\"scenario\": \"100% on final guarantees passing. Maria passed.\", \"question\": \"Did Maria get 100%?\", \"answer\": \"no\", \"reasoning_type\": \"sufficient but not necessary\"},\n",
    "    {\"scenario\": \"Countries with more hospitals have higher death rates.\", \"question\": \"Do hospitals cause death?\", \"answer\": \"no\", \"reasoning_type\": \"reverse causation\"},\n",
    "    {\"scenario\": \"Fire needs heat, fuel, oxygen. All three present, fire started.\", \"question\": \"Would removing oxygen prevent fire?\", \"answer\": \"yes\", \"reasoning_type\": \"necessary conditions\"},\n",
    "    {\"scenario\": \"Twin cities, one banned smoking. 10 years later, 20% fewer lung cancer cases.\", \"question\": \"What caused the difference?\", \"answer\": \"smoking ban\", \"reasoning_type\": \"natural experiment\"},\n",
    "    {\"scenario\": \"Deforestation→erosion→silted rivers→flooding.\", \"question\": \"Could deforestation cause flooding?\", \"answer\": \"yes\", \"reasoning_type\": \"complex causal chain\"},\n",
    "    {\"scenario\": \"Survey of gym members: 90% exercise. Conclusion: 90% of population exercises.\", \"question\": \"Is this valid?\", \"answer\": \"no\", \"reasoning_type\": \"selection bias\"},\n",
    "    {\"scenario\": \"Wore lucky socks, team won. Lucky socks caused win.\", \"question\": \"Is this valid?\", \"answer\": \"no\", \"reasoning_type\": \"post hoc fallacy\"},\n",
    "    {\"scenario\": \"Front row students get better grades. Student sits in front expecting better grades.\", \"question\": \"Will grades improve?\", \"answer\": \"no\", \"reasoning_type\": \"intervention vs observation\"},\n",
    "    # Advanced causal problems\n",
    "    {\"scenario\": \"Treatment A better than B for mild and severe cases separately, but B better overall.\", \"question\": \"Is Treatment B actually better overall?\", \"answer\": \"no\", \"reasoning_type\": \"simpson's paradox\"},\n",
    "    {\"scenario\": \"Education→higher income. Education→better jobs→higher income.\", \"question\": \"Is job position a mediating variable?\", \"answer\": \"yes\", \"reasoning_type\": \"mediating variable\"},\n",
    "    {\"scenario\": \"Coach punishes players after worst performances. Players improve after punishment.\", \"question\": \"Is punishment improving performance?\", \"answer\": \"no\", \"reasoning_type\": \"regression to mean\"},\n",
    "    {\"scenario\": \"Researcher studies successful startups: all took big risks. Concludes big risks lead to success.\", \"question\": \"Is this conclusion valid?\", \"answer\": \"no\", \"reasoning_type\": \"survivorship bias\"},\n",
    "    {\"scenario\": \"In hospital, Disease A patients less likely to have Disease B. Both cause hospitalization.\", \"question\": \"Does Disease A protect against B in general population?\", \"answer\": \"no\", \"reasoning_type\": \"berkson's paradox\"},\n",
    "    {\"scenario\": \"Distance to college affects education but not earnings directly. Used to study education→earnings.\", \"question\": \"Is distance a valid instrumental variable?\", \"answer\": \"yes\", \"reasoning_type\": \"instrumental variable\"},\n",
    "    {\"scenario\": \"More police→more arrests→higher crime stats→more police funding.\", \"question\": \"Does this prove more police causes more crime?\", \"answer\": \"no\", \"reasoning_type\": \"feedback loop\"},\n",
    "    {\"scenario\": \"Countries with more chocolate have more Nobel winners. Conclusion: chocolate makes individuals smarter.\", \"question\": \"Is this individual-level conclusion valid?\", \"answer\": \"no\", \"reasoning_type\": \"ecological fallacy\"},\n",
    "    {\"scenario\": \"Firefighters present at larger fires. Conclusion: firefighters cause larger fires.\", \"question\": \"Is this causal claim valid?\", \"answer\": \"no\", \"reasoning_type\": \"omitted variable bias\"},\n",
    "    {\"scenario\": \"Depression and social media use measured simultaneously. High correlation found.\", \"question\": \"Can we conclude social media causes depression?\", \"answer\": \"no\", \"reasoning_type\": \"temporal precedence\"},\n",
    "])\n",
    "\n",
    "_analogy_problems = pd.DataFrame([\n",
    "    {\"analogy\": \"Painter : Canvas :: Writer : ?\", \"options\": [\"Book\", \"Paper\", \"Pen\", \"Story\"], \"answer\": \"Paper\", \"relationship\": \"Artist : Medium\"},\n",
    "    {\"analogy\": \"Fish : Swim :: Bird : ?\", \"options\": [\"Feather\", \"Fly\", \"Nest\", \"Beak\"], \"answer\": \"Fly\", \"relationship\": \"Animal : Locomotion\"},\n",
    "    {\"analogy\": \"Chapter : Book :: Scene : ?\", \"options\": [\"Movie\", \"Play\", \"Actor\", \"Stage\"], \"answer\": \"Play\", \"relationship\": \"Part : Whole\"},\n",
    "    {\"analogy\": \"Thermometer : Temperature :: Speedometer : ?\", \"options\": [\"Car\", \"Velocity\", \"Dashboard\", \"Needle\"], \"answer\": \"Velocity\", \"relationship\": \"Instrument : Measurement\"},\n",
    "    {\"analogy\": \"Caterpillar : Butterfly :: Tadpole : ?\", \"options\": [\"Fish\", \"Pond\", \"Frog\", \"Egg\"], \"answer\": \"Frog\", \"relationship\": \"Juvenile : Adult\"},\n",
    "    {\"analogy\": \"Doctor : Hospital :: Teacher : ?\", \"options\": [\"Student\", \"School\", \"Classroom\", \"Education\"], \"answer\": \"School\", \"relationship\": \"Professional : Workplace\"},\n",
    "    # Advanced analogies\n",
    "    {\"analogy\": \"Sword : Knight :: Stethoscope : ?\", \"options\": [\"Hospital\", \"Doctor\", \"Patient\", \"Medicine\"], \"answer\": \"Doctor\", \"relationship\": \"Tool : Professional\"},\n",
    "    {\"analogy\": \"Telescope : Stars :: Microscope : ?\", \"options\": [\"Bacteria\", \"Laboratory\", \"Scientist\", \"Glass\"], \"answer\": \"Bacteria\", \"relationship\": \"Instrument : Observed\"},\n",
    "    {\"analogy\": \"Conductor : Orchestra :: Director : ?\", \"options\": [\"Film\", \"Actor\", \"Camera\", \"Script\"], \"answer\": \"Film\", \"relationship\": \"Leader : Creation\"},\n",
    "    {\"analogy\": \"Hunger : Eat :: Thirst : ?\", \"options\": [\"Water\", \"Drink\", \"Throat\", \"Dehydration\"], \"answer\": \"Drink\", \"relationship\": \"Sensation : Action\"},\n",
    "    {\"analogy\": \"Seed : Tree :: Egg : ?\", \"options\": [\"Nest\", \"Bird\", \"Shell\", \"Hatch\"], \"answer\": \"Bird\", \"relationship\": \"Beginning : Developed\"},\n",
    "    {\"analogy\": \"Petal : Flower :: Feather : ?\", \"options\": [\"Wing\", \"Bird\", \"Flight\", \"Nest\"], \"answer\": \"Bird\", \"relationship\": \"Part : Whole\"},\n",
    "    {\"analogy\": \"Verse : Poem :: Movement : ?\", \"options\": [\"Dance\", \"Symphony\", \"Orchestra\", \"Conductor\"], \"answer\": \"Symphony\", \"relationship\": \"Section : Work\"},\n",
    "    {\"analogy\": \"Author : Novel :: Composer : ?\", \"options\": [\"Music\", \"Symphony\", \"Piano\", \"Orchestra\"], \"answer\": \"Symphony\", \"relationship\": \"Creator : Work\"},\n",
    "    {\"analogy\": \"Dilute : Concentrate :: Expand : ?\", \"options\": [\"Contract\", \"Grow\", \"Stretch\", \"Increase\"], \"answer\": \"Contract\", \"relationship\": \"Antonyms\"},\n",
    "    {\"analogy\": \"Marathon : Sprint :: Novel : ?\", \"options\": [\"Book\", \"Story\", \"Short Story\", \"Author\"], \"answer\": \"Short Story\", \"relationship\": \"Long : Short\"},\n",
    "])\n",
    "\n",
    "def create_combined_dataset(logic_problems, math_problems, causal_problems, analogy_problems):\n",
    "    \"\"\"Create a unified reasoning evaluation dataset.\"\"\"\n",
    "    \n",
    "    # Add task_type column to each dataset\n",
    "    logic_df = logic_problems.copy()\n",
    "    logic_df['task_type'] = 'logical_deduction'\n",
    "    logic_df['prompt_template'] = logic_df.apply(\n",
    "        lambda r: f\"Premises: {r['premises']}\\nQuestion: {r['question']}\", axis=1\n",
    "    )\n",
    "    logic_df['expected'] = logic_df['answer']\n",
    "    \n",
    "    math_df = math_problems.copy()\n",
    "    math_df['task_type'] = 'math_reasoning'\n",
    "    math_df['prompt_template'] = math_df['problem']\n",
    "    math_df['expected'] = math_df['answer'].astype(str)\n",
    "    \n",
    "    causal_df = causal_problems.copy()\n",
    "    causal_df['task_type'] = 'causal_reasoning'\n",
    "    causal_df['prompt_template'] = causal_df.apply(\n",
    "        lambda r: f\"Scenario: {r['scenario']}\\nQuestion: {r['question']}\", axis=1\n",
    "    )\n",
    "    causal_df['expected'] = causal_df['answer']\n",
    "    \n",
    "    analogy_df = analogy_problems.copy()\n",
    "    analogy_df['task_type'] = 'analogical_reasoning'\n",
    "    analogy_df['prompt_template'] = analogy_df.apply(\n",
    "        lambda r: f\"Analogy: {r['analogy']}\\nOptions: {', '.join(r['options'])}\", axis=1\n",
    "    )\n",
    "    analogy_df['expected'] = analogy_df['answer']\n",
    "    \n",
    "    # Select common columns\n",
    "    common_cols = ['task_type', 'prompt_template', 'expected']\n",
    "    \n",
    "    combined = pd.concat([\n",
    "        logic_df[common_cols],\n",
    "        math_df[common_cols],\n",
    "        causal_df[common_cols],\n",
    "        analogy_df[common_cols],\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "combined_dataset = create_combined_dataset(_logic_problems, _math_problems, _causal_problems, _analogy_problems)\n",
    "print(f\"Combined dataset: {len(combined_dataset)} reasoning problems\")\n",
    "print(f\"\\nTask type distribution:\")\n",
    "print(combined_dataset['task_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57beb669",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kbench.task(store_task=False)\n",
    "def single_reasoning_task(llm, task_type: str, prompt_template: str, expected: str) -> dict:\n",
    "    \"\"\"\n",
    "    Single reasoning task for dataset evaluation.\n",
    "    \"\"\"\n",
    "    system_prompts = {\n",
    "        'logical_deduction': \"You are a logical reasoning expert. Answer with 'yes' or 'no' and explain your reasoning.\",\n",
    "        'math_reasoning': \"You are a math expert. Solve step by step and provide the final numerical answer.\",\n",
    "        'causal_reasoning': \"You are an expert in causal analysis. Identify cause-and-effect relationships.\",\n",
    "        'analogical_reasoning': \"You are an expert in pattern recognition. Complete the analogy by selecting the best option.\",\n",
    "    }\n",
    "    \n",
    "    system_prompt = system_prompts.get(task_type, \"Answer the following question.\")\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{prompt_template}\\n\\nProvide a clear, concise answer.\"\n",
    "    \n",
    "    response = llm.prompt(full_prompt)\n",
    "    \n",
    "    # Check if expected answer is in response (flexible matching)\n",
    "    is_correct = expected.lower() in response.lower()\n",
    "    \n",
    "    return {\n",
    "        \"task_type\": task_type,\n",
    "        \"prompt\": prompt_template[:100] + \"...\",\n",
    "        \"expected\": expected,\n",
    "        \"response\": response[:200],\n",
    "        \"is_correct\": is_correct,\n",
    "    }\n",
    "\n",
    "\n",
    "@kbench.task(name=\"comprehensive_reasoning_benchmark\")\n",
    "def comprehensive_reasoning_benchmark(llm, df: pd.DataFrame) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Benchmark evaluating logical, mathematical, causal, and analogical reasoning. Returns (accuracy, std_dev).\n",
    "    \"\"\"\n",
    "    with kbench.client.enable_cache():\n",
    "        runs = single_reasoning_task.evaluate(\n",
    "            stop_condition=lambda runs: len(runs) == df.shape[0],\n",
    "            max_attempts=1,\n",
    "            llm=[llm],\n",
    "            evaluation_data=df,\n",
    "            n_jobs=2,\n",
    "            timeout=120,\n",
    "            remove_run_files=True,\n",
    "        )\n",
    "    \n",
    "    eval_df = runs.as_dataframe()\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = float(eval_df['result'].apply(lambda x: x.get('is_correct', False)).mean())\n",
    "    std = float(eval_df['result'].apply(lambda x: x.get('is_correct', False)).std())\n",
    "    \n",
    "    # Calculate per-task-type accuracy\n",
    "    task_types = eval_df['result'].apply(lambda x: x.get('task_type', 'unknown'))\n",
    "    correctness = eval_df['result'].apply(lambda x: x.get('is_correct', False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"REASONING BENCHMARK RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2%} (±{std:.2%})\")\n",
    "    print(f\"\\nPer-Category Breakdown:\")\n",
    "    \n",
    "    for task_type in task_types.unique():\n",
    "        mask = task_types == task_type\n",
    "        type_accuracy = correctness[mask].mean()\n",
    "        print(f\"  • {task_type}: {type_accuracy:.2%}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Record assertions for visibility\n",
    "    kbench.assertions.assert_true(\n",
    "        accuracy > 0,\n",
    "        expectation=f\"Overall reasoning accuracy: {accuracy:.2%}\"\n",
    "    )\n",
    "    \n",
    "    return accuracy, std\n",
    "\n",
    "\n",
    "# Run the comprehensive benchmark\n",
    "run = comprehensive_reasoning_benchmark.run(\n",
    "    llm=kbench.llm,\n",
    "    df=combined_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bfc2b",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 7: Advanced - Judge LLM Evaluation for Open-Ended Reasoning\n",
    "\n",
    "Uses a judge LLM to evaluate quality of reasoning explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b005e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kbench.task(name=\"reasoning_quality_evaluation\")\n",
    "def evaluate_reasoning_quality(llm):\n",
    "    \"\"\"\n",
    "    Evaluates the QUALITY of reasoning, not just correctness.\n",
    "    \n",
    "    Uses a judge LLM to assess:\n",
    "    - Logical coherence\n",
    "    - Step-by-step clarity\n",
    "    - Correct identification of key concepts\n",
    "    - Absence of logical fallacies\n",
    "    \"\"\"\n",
    "    # Complex reasoning problem\n",
    "    problem = \"\"\"\n",
    "    Consider the following scenario:\n",
    "    \n",
    "    A company has 100 employees. 60% are engineers, 30% are managers, and 10% are executives.\n",
    "    All executives are also managers. 50% of engineers have a master's degree.\n",
    "    20% of managers (not counting executives) have a PhD.\n",
    "    All executives have either a master's or PhD.\n",
    "    \n",
    "    Question: What percentage of the company has at least a master's degree?\n",
    "    Show your reasoning step by step.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.prompt(problem)\n",
    "    \n",
    "    # Use judge LLM to evaluate the reasoning quality\n",
    "    assessment = kbench.assertions.assess_response_with_judge(\n",
    "        criteria=[\n",
    "            \"The response correctly identifies that executives are a subset of managers (not additive).\",\n",
    "            \"The response correctly calculates engineers with master's: 60% × 50% = 30%.\",\n",
    "            \"The response correctly accounts for manager degrees separately from executive degrees.\",\n",
    "            \"The response shows clear step-by-step mathematical reasoning.\",\n",
    "            \"The response arrives at a plausible final answer with proper justification.\",\n",
    "            \"The response avoids double-counting any group.\",\n",
    "        ],\n",
    "        response_text=response,\n",
    "        judge_llm=kbench.judge_llm,\n",
    "    )\n",
    "    \n",
    "    # Record each criterion as an assertion\n",
    "    for result in assessment.results:\n",
    "        kbench.assertions.assert_true(\n",
    "            result.passed,\n",
    "            expectation=f\"Criterion: {result.criterion}. Evaluation: {result.reason}\"\n",
    "        )\n",
    "    \n",
    "    # Calculate pass rate\n",
    "    passed_count = sum(1 for r in assessment.results if r.passed)\n",
    "    total_count = len(assessment.results)\n",
    "    \n",
    "    print(f\"\\nReasoning Quality Score: {passed_count}/{total_count} criteria passed\")\n",
    "\n",
    "\n",
    "# Run the quality evaluation\n",
    "evaluate_reasoning_quality.run(llm=kbench.llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51281f55",
   "metadata": {},
   "source": [
    "---\n",
    "## 📊 Final: Select Main Task for Leaderboard\n",
    "\n",
    "Choose which task to publish to the Kaggle Benchmarks leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cf664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the comprehensive benchmark as the main task for the leaderboard\n",
    "%choose comprehensive_reasoning_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca10d4",
   "metadata": {},
   "source": [
    "---\n",
    "## 📝 Summary\n",
    "\n",
    "This benchmark evaluates LLM reasoning capabilities across:\n",
    "\n",
    "| Category | # Problems | Description |\n",
    "|----------|------------|-------------|\n",
    "| Logical Deduction | 5 | Syllogisms, logical inference |\n",
    "| Math Word Problems | 5 | Multi-step calculations |\n",
    "| Causal Reasoning | 5 | Cause-effect, counterfactuals |\n",
    "| Analogical Reasoning | 6 | Pattern completion |\n",
    "| **Total** | **21** | Comprehensive reasoning evaluation |\n",
    "\n",
    "### Making This Benchmark Public\n",
    "\n",
    "1. Click **\"Save Version\"** in the notebook\n",
    "2. Go to **Settings** → **Visibility** → **Public**\n",
    "3. Add more models via \"Evaluate More Models\" button\n",
    "4. Share your benchmark URL!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Benchmarking! 🚀**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
